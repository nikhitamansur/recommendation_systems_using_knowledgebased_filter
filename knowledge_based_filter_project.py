# -*- coding: utf-8 -*-
"""knowledge_based_filter_project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1T_wEcay2xKPzhT2TJZ6LMf1eXwE9XFoz
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, MinMaxScaler
from keras.models import Sequential
from keras.layers import LSTM, Dense, Dropout
import matplotlib.pyplot as plt

# Load the data
file_path = '/content/SuperStore_Sales_DataSet.csv'
df = pd.read_csv(file_path)
print(df.head())

print(df['Quantity'])

# Data Preprocessing
data = df.dropna()  # Drop missing values

# Encode categorical features
encoder = LabelEncoder()
data['Category'] = encoder.fit_transform(data['Category'])

# Feature selection and scaling
features = ['Sales', 'Quantity', 'Profit']  # Adjust features as necessary
scaler = MinMaxScaler()
data[features] = scaler.fit_transform(data[features])

# Prepare sequences for LSTM
sequence_length = 5
X = []
y = []

for i in range(len(data) - sequence_length):
    X.append(data[features].iloc[i:i + sequence_length].values)
    y.append(data['Category'].iloc[i + sequence_length])

X = np.array(X)
y = np.array(y)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Build LSTM Model
model = Sequential()
model.add(LSTM(64, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True))
model.add(Dropout(0.2))
model.add(LSTM(32, return_sequences=False))
model.add(Dropout(0.2))
model.add(Dense(len(np.unique(y)), activation='softmax'))

# Compile the model
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Train the model
history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))

# Plot training history
plt.plot(history.history['accuracy'], label='Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

# Evaluate the model
loss, accuracy = model.evaluate(X_test, y_test)
print(f'Test Accuracy: {accuracy:.2f}')

# Knowledge-Based Filtering Implementation
def knowledge_based_filter(product, sales_threshold=0.5, profit_threshold=0.5):
    """
    A simple knowledge-based filter that recommends products based on business rules.
    :param product: Product name or ID.
    :param sales_threshold: Minimum sales score for recommendation.
    :param profit_threshold: Minimum profit score for recommendation.
    :return: True if the product should be recommended, False otherwise.
    """
    # Fetch product details
    product_data = data[data['Product Name'] == product]
    if product_data.empty:
        return False

    # Apply knowledge-based rules
    if product_data['Sales'].values[0] > sales_threshold and product_data['Profit'].values[0] > profit_threshold:
        return True
    return False

# Example of generating recommendations
def recommend_products():
    recommendations = []
    for product in data['Product Name'].unique():
        if knowledge_based_filter(product):
            recommendations.append(product)
    return recommendations

# Get and display recommendations
recommended_products = recommend_products()
print("Recommended Products based on Knowledge-Based Filtering:")
print(recommended_products)

import seaborn as sns
# Data Preprocessing
data = data.dropna()  # Drop missing values

# Feature selection
features = ['Sales', 'Quantity', 'Profit']

# 1. Plot Histograms for Sales, Quantity, and Profit
plt.figure(figsize=(15, 5))
for i, feature in enumerate(features):
    plt.subplot(1, 3, i + 1)
    plt.hist(data[feature], bins=30, color='blue', alpha=0.7)
    plt.title(f'{feature} Distribution')
    plt.xlabel(feature)
    plt.ylabel('Frequency')
plt.tight_layout()
plt.show()

# 2. Plot Model Training Performance
# Assuming `history` is the training history object from the LSTM model
plt.figure(figsize=(10, 5))

# Plot Accuracy
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Training Accuracy', color='blue')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy', color='orange')
plt.title('Model Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

# Plot Loss
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Training Loss', color='blue')
plt.plot(history.history['val_loss'], label='Validation Loss', color='orange')
plt.title('Model Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.show()

# 3. Sales vs. Profit Scatter Plot
plt.figure(figsize=(8, 6))
sns.scatterplot(x='Sales', y='Profit', data=data, alpha=0.6, edgecolor=None)
plt.title('Sales vs. Profit')
plt.xlabel('Sales')
plt.ylabel('Profit')
plt.show()

pip install scikit-learn

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score
import seaborn as sns

# Function to generate confusion matrix for recommender system
def evaluate_and_plot_confusion_matrix(true_recommendations, predicted_recommendations):
    """
    Evaluate the recommender system and print the confusion matrix.

    :param true_recommendations: List of true products that should be recommended (ground truth)
    :param predicted_recommendations: List of products recommended by the system
    :return: None (prints out the confusion matrix and evaluation metrics)
    """
    # Get all unique products
    all_products = set(true_recommendations + predicted_recommendations)

    # Convert the lists to binary format (1 if product is recommended, 0 otherwise)
    true_binary = [1 if product in true_recommendations else 0 for product in all_products]
    predicted_binary = [1 if product in predicted_recommendations else 0 for product in all_products]

    # Generate confusion matrix
    cm = confusion_matrix(true_binary, predicted_binary)

    # Calculate evaluation metrics
    precision = precision_score(true_binary, predicted_binary)
    recall = recall_score(true_binary, predicted_binary)
    f1 = f1_score(true_binary, predicted_binary)
    accuracy = accuracy_score(true_binary, predicted_binary)

    # Print evaluation metrics
    print("Recommender System Evaluation Metrics:")
    print(f"Precision: {precision:.4f}")
    print(f"Recall: {recall:.4f}")
    print(f"F1-Score: {f1:.4f}")
    print(f"Accuracy: {accuracy:.4f}")

    # Coverage (Proportion of products recommended out of all possible products)
    coverage = len(predicted_recommendations) / len(all_products)
    print(f"Coverage: {coverage:.4f}")

    # Plot confusion matrix
    plt.figure(figsize=(6, 5))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=['Not Recommended', 'Recommended'], yticklabels=['Not Recommended', 'Recommended'])
    plt.title("Confusion Matrix")
    plt.xlabel("Predicted")
    plt.ylabel("True")
    plt.show()

# Example Usage
# Assume these are your true and predicted recommendations
true_recommendations = ["Product A", "Product B", "Product C"]  # Example ground truth
predicted_recommendations = ["Product A", "Product D", "Product E"]  # Example predictions

# Evaluate the recommender system and plot the confusion matrix
evaluate_and_plot_confusion_matrix(true_recommendations, predicted_recommendations)